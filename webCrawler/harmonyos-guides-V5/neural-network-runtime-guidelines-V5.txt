Neural Network Runtime对接AI推理框架开发指导
场景介绍
Neural Network Runtime作为AI推理引擎和加速芯片的桥梁，为AI推理引擎提供精简的Native接口，满足推理引擎通过加速芯片执行端到端推理的需求。
本文以图1展示的Add单算子模型为例，介绍Neural Network Runtime的开发流程。Add算子包含两个输入、一个参数和一个输出，其中的activation参数用于指定Add算子中激活函数的类型。
图1 Add单算子网络示意图
环境准备
[h2]环境要求
Neural Network Runtime部件的环境要求如下：
开发环境：Ubuntu 18.04及以上。接入设备：系统定义的标准设备，系统中内置AI硬件驱动并已接入Neural Network Runtime。
由于Neural Network Runtime通过Native API对外开放，需要通过Native开发套件编译Neural Network Runtime应用。在社区的每日构建中下载对应系统版本的ohos-sdk压缩包，从压缩包中提取对应平台的Native开发套件。以Linux为例，Native开发套件的压缩包命名为native-linux-{版本号}.zip。
[h2]环境搭建
打开Ubuntu编译服务器的终端。 把下载好的Native开发套件压缩包拷贝至当前用户根目录下。 执行以下命令解压Native开发套件的压缩包。 unzip native-linux-{版本号}.zip  解压缩后的内容如下（随版本迭代，目录下的内容可能发生变化，请以最新版本的Native API为准）： native/\n├── build // 交叉编译工具链\n├── build-tools // 编译构建工具\n├── docs\n├── llvm\n├── nativeapi_syscap_config.json\n├── ndk_system_capability.json\n├── NOTICE.txt\n├── oh-uni-package.json\n└── sysroot // Native API头文件和库 
接口说明
这里给出Neural Network Runtime开发流程中通用的接口，具体请见下列表格。
[h2]结构体
[h2]模型构造接口
[h2]模型编译接口
[h2]张量描述接口
[h2]张量接口
[h2]执行推理接口
[h2]设备管理接口
开发步骤
Neural Network Runtime的开发流程主要包含模型构造、模型编译和推理执行三个阶段。以下开发步骤以Add单算子模型为例，介绍调用Neural Network Runtime接口，开发应用的过程。
创建应用样例文件。  首先，创建Neural Network Runtime应用样例的源文件。在项目目录下执行以下命令，创建nnrt_example/目录，并在目录下创建 nnrt_example.cpp 源文件。 mkdir ~/nnrt_example && cd ~/nnrt_example\ntouch nnrt_example.cpp 导入Neural Network Runtime。  在 nnrt_example.cpp 文件的开头添加以下代码，引入Neural Network Runtime。 #include <iostream>\n#include <cstdarg>\n#include \"neural_network_runtime/neural_network_runtime.h\" 定义日志打印、设置输入数据、数据打印等辅助函数。 // 返回值检查宏\n#define CHECKNEQ(realRet, expectRet, retValue, ...) \\\n    do { \\\n        if ((realRet) != (expectRet)) { \\\n            printf(__VA_ARGS__); \\\n            return (retValue); \\\n        } \\\n    } while (0)\n\n#define CHECKEQ(realRet, expectRet, retValue, ...) \\\n    do { \\\n        if ((realRet) == (expectRet)) { \\\n            printf(__VA_ARGS__); \\\n            return (retValue); \\\n        } \\\n    } while (0)\n\n// 设置输入数据用于推理\nOH_NN_ReturnCode SetInputData(NN_Tensor* inputTensor[], size_t inputSize)\n{\n    OH_NN_DataType dataType(OH_NN_FLOAT32);\n    OH_NN_ReturnCode ret{OH_NN_FAILED};\n    size_t elementCount = 0;\n    for (size_t i = 0; i < inputSize; ++i) {\n        // 获取张量的数据内存\n        auto data = OH_NNTensor_GetDataBuffer(inputTensor[i]);\n        CHECKEQ(data, nullptr, OH_NN_FAILED, \"Failed to get data buffer.\");\n        // 获取张量的描述\n        auto desc = OH_NNTensor_GetTensorDesc(inputTensor[i]);\n        CHECKEQ(desc, nullptr, OH_NN_FAILED, \"Failed to get desc.\");\n        // 获取张量的数据类型\n        ret = OH_NNTensorDesc_GetDataType(desc, &dataType);\n        CHECKNEQ(ret, OH_NN_SUCCESS, OH_NN_FAILED, \"Failed to get data type.\");\n        // 获取张量的元素个数\n        ret = OH_NNTensorDesc_GetElementCount(desc, &elementCount);\n        CHECKNEQ(ret, OH_NN_SUCCESS, OH_NN_FAILED, \"Failed to get element count.\");\n        switch(dataType) {\n            case OH_NN_FLOAT32: {\n                float* floatValue = reinterpret_cast<float*>(data);\n                for (size_t j = 0; j < elementCount; ++j) {\n                    floatValue[j] = static_cast<float>(j);\n                }\n                break;\n            }\n            case OH_NN_INT32: {\n                int* intValue = reinterpret_cast<int*>(data);\n                for (size_t j = 0; j < elementCount; ++j) {\n                    intValue[j] = static_cast<int>(j);\n                }\n                break;\n            }\n            default:\n                return OH_NN_FAILED;\n        }\n    }\n    return OH_NN_SUCCESS;\n}\n\nOH_NN_ReturnCode Print(NN_Tensor* outputTensor[], size_t outputSize)\n{\n    OH_NN_DataType dataType(OH_NN_FLOAT32);\n    OH_NN_ReturnCode ret{OH_NN_FAILED};\n    size_t elementCount = 0;\n    for (size_t i = 0; i < outputSize; ++i) {\n        auto data = OH_NNTensor_GetDataBuffer(outputTensor[i]);\n        CHECKEQ(data, nullptr, OH_NN_FAILED, \"Failed to get data buffer.\");\n        auto desc = OH_NNTensor_GetTensorDesc(outputTensor[i]);\n        CHECKEQ(desc, nullptr, OH_NN_FAILED, \"Failed to get desc.\");\n        ret = OH_NNTensorDesc_GetDataType(desc, &dataType);\n        CHECKNEQ(ret, OH_NN_SUCCESS, OH_NN_FAILED, \"Failed to get data type.\");\n        ret = OH_NNTensorDesc_GetElementCount(desc, &elementCount);\n        CHECKNEQ(ret, OH_NN_SUCCESS, OH_NN_FAILED, \"Failed to get element count.\");\n        switch(dataType) {\n            case OH_NN_FLOAT32: {\n                float* floatValue = reinterpret_cast<float*>(data);\n                for (size_t j = 0; j < elementCount; ++j) {\n                    std::cout << \"Output index: \" << j << \", value is: \" << floatValue[j] << \".\" << std::endl;\n                }\n                break;\n            }\n            case OH_NN_INT32: {\n                int* intValue = reinterpret_cast<int*>(data);\n                for (size_t j = 0; j < elementCount; ++j) {\n                    std::cout << \"Output index: \" << j << \", value is: \" << intValue[j] << \".\" << std::endl;\n                }\n                break;\n            }\n            default:\n                return OH_NN_FAILED;\n        }\n    }\n\n    return OH_NN_SUCCESS;\n} 构造模型。  使用Neural Network Runtime的模型构造接口，构造Add单算子样例模型。 OH_NN_ReturnCode BuildModel(OH_NNModel** pmodel)\n{\n    // 创建模型实例model，进行模型构造\n    OH_NNModel* model = OH_NNModel_Construct();\n    CHECKEQ(model, nullptr, OH_NN_FAILED, \"Create model failed.\");\n\n    // 添加Add算子的第一个输入张量，类型为float32，张量形状为[1, 2, 2, 3]\n    NN_TensorDesc* tensorDesc = OH_NNTensorDesc_Create();\n    CHECKEQ(tensorDesc, nullptr, OH_NN_FAILED, \"Create TensorDesc failed.\");\n\n    int32_t inputDims[4] = {1, 2, 2, 3};\n    auto returnCode = OH_NNTensorDesc_SetShape(tensorDesc, inputDims, 4);\n    CHECKNEQ(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, \"Set TensorDesc shape failed.\");\n\n    returnCode = OH_NNTensorDesc_SetDataType(tensorDesc, OH_NN_FLOAT32);\n    CHECKNEQ(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, \"Set TensorDesc data type failed.\");\n\n    returnCode = OH_NNTensorDesc_SetFormat(tensorDesc, OH_NN_FORMAT_NONE);\n    CHECKNEQ(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, \"Set TensorDesc format failed.\");\n\n    returnCode = OH_NNModel_AddTensorToModel(model, tensorDesc);\n    CHECKNEQ(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, \"Add first TensorDesc to model failed.\");\n\n    returnCode = OH_NNModel_SetTensorType(model, 0, OH_NN_TENSOR);\n    CHECKNEQ(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, \"Set model tensor type failed.\");\n\n    // 添加Add算子的第二个输入张量，类型为float32，张量形状为[1, 2, 2, 3]\n    tensorDesc = OH_NNTensorDesc_Create();\n    CHECKEQ(tensorDesc, nullptr, OH_NN_FAILED, \"Create TensorDesc failed.\");\n\n    returnCode = OH_NNTensorDesc_SetShape(tensorDesc, inputDims, 4);\n    CHECKNEQ(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, \"Set TensorDesc shape failed.\");\n\n    returnCode = OH_NNTensorDesc_SetDataType(tensorDesc, OH_NN_FLOAT32);\n    CHECKNEQ(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, \"Set TensorDesc data type failed.\");\n\n    returnCode = OH_NNTensorDesc_SetFormat(tensorDesc, OH_NN_FORMAT_NONE);\n    CHECKNEQ(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, \"Set TensorDesc format failed.\");\n\n    returnCode = OH_NNModel_AddTensorToModel(model, tensorDesc);\n    CHECKNEQ(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, \"Add second TensorDesc to model failed.\");\n\n    returnCode = OH_NNModel_SetTensorType(model, 1, OH_NN_TENSOR);\n    CHECKNEQ(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, \"Set model tensor type failed.\");\n\n    // 添加Add算子的参数张量，该参数张量用于指定激活函数的类型，张量的数据类型为int8。\n    tensorDesc = OH_NNTensorDesc_Create();\n    CHECKEQ(tensorDesc, nullptr, OH_NN_FAILED, \"Create TensorDesc failed.\");\n\n    int32_t activationDims = 1;\n    returnCode = OH_NNTensorDesc_SetShape(tensorDesc, &activationDims, 1);\n    CHECKNEQ(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, \"Set TensorDesc shape failed.\");\n\n    returnCode = OH_NNTensorDesc_SetDataType(tensorDesc, OH_NN_INT8);\n    CHECKNEQ(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, \"Set TensorDesc data type failed.\");\n\n    returnCode = OH_NNTensorDesc_SetFormat(tensorDesc, OH_NN_FORMAT_NONE);\n    CHECKNEQ(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, \"Set TensorDesc format failed.\");\n\n    returnCode = OH_NNModel_AddTensorToModel(model, tensorDesc);\n    CHECKNEQ(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, \"Add second TensorDesc to model failed.\");\n\n    returnCode = OH_NNModel_SetTensorType(model, 2, OH_NN_ADD_ACTIVATIONTYPE);\n    CHECKNEQ(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, \"Set model tensor type failed.\");\n\n    // 将激活函数类型设置为OH_NN_FUSED_NONE，表示该算子不添加激活函数。\n    int8_t activationValue = OH_NN_FUSED_NONE;\n    returnCode = OH_NNModel_SetTensorData(model, 2, &activationValue, sizeof(int8_t));\n    CHECKNEQ(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, \"Set model tensor data failed.\");\n\n    // 设置Add算子的输出张量，类型为float32，张量形状为[1, 2, 2, 3]\n    tensorDesc = OH_NNTensorDesc_Create();\n    CHECKEQ(tensorDesc, nullptr, OH_NN_FAILED, \"Create TensorDesc failed.\");\n\n    returnCode = OH_NNTensorDesc_SetShape(tensorDesc, inputDims, 4);\n    CHECKNEQ(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, \"Set TensorDesc shape failed.\");\n\n    returnCode = OH_NNTensorDesc_SetDataType(tensorDesc, OH_NN_FLOAT32);\n    CHECKNEQ(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, \"Set TensorDesc data type failed.\");\n\n    returnCode = OH_NNTensorDesc_SetFormat(tensorDesc, OH_NN_FORMAT_NONE);\n    CHECKNEQ(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, \"Set TensorDesc format failed.\");\n\n    returnCode = OH_NNModel_AddTensorToModel(model, tensorDesc);\n    CHECKNEQ(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, \"Add forth TensorDesc to model failed.\");\n\n    returnCode = OH_NNModel_SetTensorType(model, 3, OH_NN_TENSOR);\n    CHECKNEQ(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, \"Set model tensor type failed.\");\n\n    // 指定Add算子的输入张量、参数张量和输出张量的索引\n    uint32_t inputIndicesValues[2] = {0, 1};\n    uint32_t paramIndicesValues = 2;\n    uint32_t outputIndicesValues = 3;\n    OH_NN_UInt32Array paramIndices = {&paramIndicesValues, 1};\n    OH_NN_UInt32Array inputIndices = {inputIndicesValues, 2};\n    OH_NN_UInt32Array outputIndices = {&outputIndicesValues, 1};\n\n    // 向模型实例添加Add算子\n    returnCode = OH_NNModel_AddOperation(model, OH_NN_OPS_ADD, &paramIndices, &inputIndices, &outputIndices);\n    CHECKNEQ(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, \"Add operation to model failed.\");\n\n    // 设置模型实例的输入张量、输出张量的索引\n    returnCode = OH_NNModel_SpecifyInputsAndOutputs(model, &inputIndices, &outputIndices);\n    CHECKNEQ(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, \"Specify model inputs and outputs failed.\");\n\n    // 完成模型实例的构建\n    returnCode = OH_NNModel_Finish(model);\n    CHECKNEQ(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, \"Build model failed.\");\n\n    // 返回模型实例\n    *pmodel = model;\n    return OH_NN_SUCCESS;\n} 查询Neural Network Runtime已经对接的AI加速芯片。  Neural Network Runtime支持通过HDI接口，对接多种AI加速芯片。在执行模型编译前，需要查询当前设备下，Neural Network Runtime已经对接的AI加速芯片。每个AI加速芯片对应唯一的ID值，在编译阶段需要通过设备ID，指定模型编译的芯片。 void GetAvailableDevices(std::vector<size_t>& availableDevice)\n{\n    availableDevice.clear();\n\n    // 获取可用的硬件ID\n    const size_t* devices = nullptr;\n    uint32_t deviceCount = 0;\n    OH_NN_ReturnCode ret = OH_NNDevice_GetAllDevicesID(&devices, &deviceCount);\n    if (ret != OH_NN_SUCCESS) {\n        std::cout << \"GetAllDevicesID failed, get no available device.\" << std::endl;\n        return;\n    }\n\n    for (uint32_t i = 0; i < deviceCount; i++) {\n        availableDevice.emplace_back(devices[i]);\n    }\n} 在指定的设备上编译模型。  Neural Network Runtime使用抽象的模型表达描述AI模型的拓扑结构。在AI加速芯片上执行前，需要通过Neural Network Runtime提供的编译模块来创建编译实例，并由编译实例将抽象的模型表达下发至芯片驱动层，转换成可以直接推理计算的格式，即模型编译。 OH_NN_ReturnCode CreateCompilation(OH_NNModel* model, const std::vector<size_t>& availableDevice,\n                                   OH_NNCompilation** pCompilation)\n{\n    // 创建编译实例compilation，将构图的模型实例或MSLite传下来的模型实例传入\n    OH_NNCompilation* compilation = OH_NNCompilation_Construct(model);\n    CHECKEQ(compilation, nullptr, OH_NN_FAILED, \"OH_NNCore_ConstructCompilationWithNNModel failed.\");\n\n    // 设置编译的硬件、缓存路径、性能模式、计算优先级、是否开启float16低精度计算等选项\n    // 选择在第一个设备上编译模型\n    auto returnCode = OH_NNCompilation_SetDevice(compilation, availableDevice[0]);\n    CHECKNEQ(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, \"OH_NNCompilation_SetDevice failed.\");\n\n    // 将模型编译结果缓存在/data/local/tmp目录下，版本号指定为1\n    returnCode = OH_NNCompilation_SetCache(compilation, \"/data/local/tmp\", 1);\n    CHECKNEQ(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, \"OH_NNCompilation_SetCache failed.\");\n\n    // 设置硬件性能模式\n    returnCode = OH_NNCompilation_SetPerformanceMode(compilation, OH_NN_PERFORMANCE_EXTREME);\n    CHECKNEQ(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, \"OH_NNCompilation_SetPerformanceMode failed.\");\n\n    // 设置推理执行优先级\n    returnCode = OH_NNCompilation_SetPriority(compilation, OH_NN_PRIORITY_HIGH);\n    CHECKNEQ(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, \"OH_NNCompilation_SetPriority failed.\");\n\n    // 是否开启FP16计算模式\n    returnCode = OH_NNCompilation_EnableFloat16(compilation, false);\n    CHECKNEQ(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, \"OH_NNCompilation_EnableFloat16 failed.\");\n\n    // 执行模型编译\n    returnCode = OH_NNCompilation_Build(compilation);\n    CHECKNEQ(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, \"OH_NNCompilation_Build failed.\");\n\n    *pCompilation = compilation;\n    return OH_NN_SUCCESS;\n} 创建执行器。  完成模型编译后，需要调用Neural Network Runtime的执行模块，通过编译实例创建执行器。模型推理阶段中的设置模型输入、触发推理计算以及获取模型输出等操作均需要围绕执行器完成。 OH_NNExecutor* CreateExecutor(OH_NNCompilation* compilation)\n{\n    // 通过编译实例compilation创建执行器executor\n    OH_NNExecutor *executor = OH_NNExecutor_Construct(compilation);\n    CHECKEQ(executor, nullptr, nullptr, \"OH_NNExecutor_Construct failed.\");\n    return executor;\n} 执行推理计算，并打印推理结果。  通过执行模块提供的接口，将推理计算所需要的输入数据传递给执行器，触发执行器完成一次推理计算，获取模型的推理结果并打印。 OH_NN_ReturnCode Run(OH_NNExecutor* executor, const std::vector<size_t>& availableDevice)\n{\n    // 从executor获取输入输出信息\n    // 获取输入张量的个数\n    size_t inputCount = 0;\n    auto returnCode = OH_NNExecutor_GetInputCount(executor, &inputCount);\n    CHECKNEQ(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, \"OH_NNExecutor_GetInputCount failed.\");\n    std::vector<NN_TensorDesc*> inputTensorDescs;\n    NN_TensorDesc* tensorDescTmp = nullptr;\n    for (size_t i = 0; i < inputCount; ++i) {\n        // 创建输入张量的描述\n        tensorDescTmp = OH_NNExecutor_CreateInputTensorDesc(executor, i);\n        CHECKEQ(tensorDescTmp, nullptr, OH_NN_FAILED, \"OH_NNExecutor_CreateInputTensorDesc failed.\");\n        inputTensorDescs.emplace_back(tensorDescTmp);\n    }\n    // 获取输出张量的个数\n    size_t outputCount = 0;\n    returnCode = OH_NNExecutor_GetOutputCount(executor, &outputCount);\n    CHECKNEQ(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, \"OH_NNExecutor_GetOutputCount failed.\");\n    std::vector<NN_TensorDesc*> outputTensorDescs;\n    for (size_t i = 0; i < outputCount; ++i) {\n        // 创建输出张量的描述\n        tensorDescTmp = OH_NNExecutor_CreateOutputTensorDesc(executor, i);\n        CHECKEQ(tensorDescTmp, nullptr, OH_NN_FAILED, \"OH_NNExecutor_CreateOutputTensorDesc failed.\");\n        outputTensorDescs.emplace_back(tensorDescTmp);\n    }\n\n    // 创建输入和输出张量\n    NN_Tensor* inputTensors[inputCount];\n    NN_Tensor* tensor = nullptr;\n    for (size_t i = 0; i < inputCount; ++i) {\n        tensor = nullptr;\n        tensor = OH_NNTensor_Create(availableDevice[0], inputTensorDescs[i]);\n        CHECKEQ(tensor, nullptr, OH_NN_FAILED, \"OH_NNTensor_Create failed.\");\n        inputTensors[i] = tensor;\n    }\n    NN_Tensor* outputTensors[outputCount];\n    for (size_t i = 0; i < outputCount; ++i) {\n        tensor = nullptr;\n        tensor = OH_NNTensor_Create(availableDevice[0], outputTensorDescs[i]);\n        CHECKEQ(tensor, nullptr, OH_NN_FAILED, \"OH_NNTensor_Create failed.\");\n        outputTensors[i] = tensor;\n    }\n\n    // 设置输入张量的数据\n    returnCode = SetInputData(inputTensors, inputCount);\n    CHECKNEQ(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, \"SetInputData failed.\");\n\n    // 执行推理\n    returnCode = OH_NNExecutor_RunSync(executor, inputTensors, inputCount, outputTensors, outputCount);\n    CHECKNEQ(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, \"OH_NNExecutor_RunSync failed.\");\n\n    // 打印输出张量的数据\n    Print(outputTensors, outputCount);\n\n    // 清理输入和输出张量以及张量描述\n    for (size_t i = 0; i < inputCount; ++i) {\n        returnCode = OH_NNTensor_Destroy(&inputTensors[i]);\n        CHECKNEQ(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, \"OH_NNTensor_Destroy failed.\");\n        returnCode = OH_NNTensorDesc_Destroy(&inputTensorDescs[i]);\n        CHECKNEQ(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, \"OH_NNTensorDesc_Destroy failed.\");\n    }\n    for (size_t i = 0; i < outputCount; ++i) {\n        returnCode = OH_NNTensor_Destroy(&outputTensors[i]);\n        CHECKNEQ(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, \"OH_NNTensor_Destroy failed.\");\n        returnCode = OH_NNTensorDesc_Destroy(&outputTensorDescs[i]);\n        CHECKNEQ(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, \"OH_NNTensorDesc_Destroy failed.\");\n    }\n\n    return OH_NN_SUCCESS;\n} 构建端到端模型构造-编译-执行流程。  步骤4-步骤8实现了模型的模型构造、编译和执行流程，并封装成多个函数，便于模块化开发。以下示例代码将串联这些函数， 形成一个完整的Neural Network Runtime使用流程。 int main(int argc, char** argv)\n{\n    OH_NNModel* model = nullptr;\n    OH_NNCompilation* compilation = nullptr;\n    OH_NNExecutor* executor = nullptr;\n    std::vector<size_t> availableDevices;\n\n    // 模型构造\n    OH_NN_ReturnCode ret = BuildModel(&model);\n    if (ret != OH_NN_SUCCESS) {\n        std::cout << \"BuildModel failed.\" << std::endl;\n        OH_NNModel_Destroy(&model);\n        return -1;\n    }\n\n    // 获取可执行的设备\n    GetAvailableDevices(availableDevices);\n    if (availableDevices.empty()) {\n        std::cout << \"No available device.\" << std::endl;\n        OH_NNModel_Destroy(&model);\n        return -1;\n    }\n\n    // 模型编译\n    ret = CreateCompilation(model, availableDevices, &compilation);\n    if (ret != OH_NN_SUCCESS) {\n        std::cout << \"CreateCompilation failed.\" << std::endl;\n        OH_NNModel_Destroy(&model);\n        OH_NNCompilation_Destroy(&compilation);\n        return -1;\n    }\n\n    // 销毁模型实例\n    OH_NNModel_Destroy(&model);\n\n    // 创建模型的推理执行器\n    executor = CreateExecutor(compilation);\n    if (executor == nullptr) {\n        std::cout << \"CreateExecutor failed, no executor is created.\" << std::endl;\n        OH_NNCompilation_Destroy(&compilation);\n        return -1;\n    }\n\n    // 销毁编译实例\n    OH_NNCompilation_Destroy(&compilation);\n\n    // 使用上一步创建的执行器，执行推理计算\n    ret = Run(executor, availableDevices);\n    if (ret != OH_NN_SUCCESS) {\n        std::cout << \"Run failed.\" << std::endl;\n        OH_NNExecutor_Destroy(&executor);\n        return -1;\n    }\n\n    // 销毁执行器实例\n    OH_NNExecutor_Destroy(&executor);\n\n    return 0;\n} 
调测验证
准备应用样例的编译配置文件。  新建一个 CMakeLists.txt 文件，为开发步骤中的应用样例文件 nnrt_example.cpp 添加编译配置。以下提供简单的 CMakeLists.txt 示例： cmake_minimum_required(VERSION 3.16)\nproject(nnrt_example C CXX)\n\nadd_executable(nnrt_example\n    ./nnrt_example.cpp\n)\n\ntarget_link_libraries(nnrt_example\n    neural_network_runtime\n    neural_network_core\n) 编译应用样例。  执行以下命令，在当前目录下新建build/目录，在build/目录下编译 nnrt_example.cpp，得到二进制文件 nnrt_example。 mkdir build && cd build\ncmake -DCMAKE_TOOLCHAIN_FILE={交叉编译工具链的路径}/build/cmake/ohos.toolchain.cmake -DOHOS_ARCH=arm64-v8a -DOHOS_PLATFORM=OHOS -DOHOS_STL=c++_static ..\nmake 执行以下代码，将样例推送到设备上执行。 # 将编译得到的 `nnrt_example` 推送到设备上，执行样例。\nhdc_std file send ./nnrt_example /data/local/tmp/.\n\n# 给测试用例可执行文件加上权限。\nhdc_std shell \"chmod +x /data/local/tmp/nnrt_example\"\n\n# 执行测试用例\nhdc_std shell \"/data/local/tmp/nnrt_example\"  如果样例执行正常，应该得到以下输出。 Output index: 0, value is: 0.000000.\nOutput index: 1, value is: 2.000000.\nOutput index: 2, value is: 4.000000.\nOutput index: 3, value is: 6.000000.\nOutput index: 4, value is: 8.000000.\nOutput index: 5, value is: 10.000000.\nOutput index: 6, value is: 12.000000.\nOutput index: 7, value is: 14.000000.\nOutput index: 8, value is: 16.000000.\nOutput index: 9, value is: 18.000000.\nOutput index: 10, value is: 20.000000.\nOutput index: 11, value is: 22.000000. 检查模型缓存（可选）。  如果在调测环境下，Neural Network Runtime对接的HDI服务支持模型缓存功能，执行完 nnrt_example, 可以在 /data/local/tmp 目录下找到生成的缓存文件。   模型的IR需要传递到硬件驱动层，由HDI服务将统一的IR图，编译成硬件专用的计算图，编译的过程非常耗时。Neural Network Runtime支持计算图缓存的特性，可以将HDI服务编译生成的计算图，缓存到设备存储中。当下一次在同一个加速芯片上编译同一个模型时，通过指定缓存的路径，Neural Network Runtime可以直接加载缓存文件中的计算图，减少编译消耗的时间。   检查缓存目录下的缓存文件： ls /data/local/tmp  以下为打印结果： # 0.nncache 1.nncache 2.nncache cache_info.nncache  如果缓存不再使用，需要手动删除缓存，可以参考以下命令，删除缓存文件。 rm /data/local/tmp/*nncache 
