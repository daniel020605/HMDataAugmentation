使用MindSpore Lite引擎进行模型推理 (C/C++)
场景介绍
MindSpore Lite是一款AI引擎，它提供了面向不同硬件设备AI模型推理的功能，目前已经在图像分类、目标识别、人脸识别、文字识别等应用中广泛使用。
本文介绍使用MindSpore Lite推理引擎进行模型推理的通用开发流程。
基本概念
在进行开发前，请先了解以下概念。
张量：它与数组和矩阵非常相似，是MindSpore Lite网络运算中的基本数据结构。
Float16推理模式： Float16又称半精度，它使用16比特表示一个数。Float16推理模式表示推理的时候用半精度进行推理。 
接口说明
这里给出MindSpore Lite推理的通用开发流程中涉及的一些接口，具体请见下列表格。
[h2]Context 相关接口
[h2]Model 相关接口
[h2]Tensor 相关接口
开发步骤
使用MindSpore Lite进行模型推理的开发流程如下图所示。
图 1 使用MindSpore Lite进行模型推理的开发流程
进入主要流程之前需要先引用相关的头文件，并编写函数生成随机的输入，具体如下：
#include <stdlib.h>\n#include <stdio.h>\n#include \"mindspore/model.h\"\n\n//生成随机的输入\nint GenerateInputDataWithRandom(OH_AI_TensorHandleArray inputs) {\n  for (size_t i = 0; i < inputs.handle_num; ++i) {\n    float *input_data = (float *)OH_AI_TensorGetMutableData(inputs.handle_list[i]);\n    if (input_data == NULL) {\n      printf(\"MSTensorGetMutableData failed.\\n\");\n      return OH_AI_STATUS_LITE_ERROR;\n    }\n    int64_t num = OH_AI_TensorGetElementNum(inputs.handle_list[i]);\n    const int divisor = 10;\n    for (size_t j = 0; j < num; j++) {\n      input_data[j] = (float)(rand() % divisor) / divisor;  // 0--0.9f\n    }\n  }\n  return OH_AI_STATUS_SUCCESS;\n}
然后进入主要的开发步骤，具括包括模型的准备、读取、编译、推理和释放，具体开发过程及细节请见下文的开发步骤及示例。
模型准备。  需要的模型可以直接下载，也可以通过模型转换工具获得。 下载模型的格式若为.ms，则可以直接使用。本文以mobilenetv2.ms为例。如果是第三方框架的模型，比如 TensorFlow、TensorFlow Lite、Caffe、ONNX等，可以使用模型转换工具转换为.ms格式的模型文件。 创建上下文，设置线程数、设备类型等参数。  以下介绍两种典型情形。  情形1：仅创建CPU推理上下文。 // 创建并配置上下文，设置运行时的线程数量为2，绑核策略为大核优先\nOH_AI_ContextHandle context = OH_AI_ContextCreate();\nif (context == NULL) {\n  printf(\"OH_AI_ContextCreate failed.\\n\");\n  return OH_AI_STATUS_LITE_ERROR;\n}\nconst int thread_num = 2;\nOH_AI_ContextSetThreadNum(context, thread_num);\nOH_AI_ContextSetThreadAffinityMode(context, 1);\n//设置运行设备为CPU，不使用Float16推理\nOH_AI_DeviceInfoHandle cpu_device_info = OH_AI_DeviceInfoCreate(OH_AI_DEVICETYPE_CPU);\nif (cpu_device_info == NULL) {\n  printf(\"OH_AI_DeviceInfoCreate failed.\\n\");\n  OH_AI_ContextDestroy(&context);\n  return OH_AI_STATUS_LITE_ERROR;\n}\nOH_AI_DeviceInfoSetEnableFP16(cpu_device_info, false);\nOH_AI_ContextAddDeviceInfo(context, cpu_device_info);  情形2：创建NNRT（Neural Network Runtime）和CPU异构推理上下文。  NNRT是面向AI领域的跨芯片推理计算运行时，一般来说，NNRT对接的加速硬件如NPU，推理能力较强，但支持的算子规格少；而通用CPU推理能力较弱，但支持算子规格更全面。MindSpore Lite支持配置NNRT硬件和CPU异构推理：优先将模型算子调度到NNRT推理，若某些算子NNRT不支持，将其调度到CPU进行推理。通过下面的操作即可配置NNRT/CPU异构推理。 // 创建并配置上下文，设置运行时的线程数量为2，绑核策略为大核优先\nOH_AI_ContextHandle context = OH_AI_ContextCreate();\nif (context == NULL) {\n  printf(\"OH_AI_ContextCreate failed.\\n\");\n  return OH_AI_STATUS_LITE_ERROR;\n}\n// 优先使用NNRT推理。\n// 这里利用查找到的第一个ACCELERATORS类别的NNRT硬件，来创建nnrt设备信息，并设置硬件使用高性能模式推理。还可以通过如：OH_AI_GetAllNNRTDeviceDescs()接口获取当前环境中所有NNRT硬件的描述信息，按设备名、类型等信息查找，找到某一具体设备作为NNRT推理硬件。\nOH_AI_DeviceInfoHandle nnrt_device_info = OH_AI_CreateNNRTDeviceInfoByType(OH_AI_NNRTDEVICE_ACCELERATOR);\nif (nnrt_device_info == NULL) {\n  printf(\"OH_AI_DeviceInfoCreate failed.\\n\");\n  OH_AI_ContextDestroy(&context);\n  return OH_AI_STATUS_LITE_ERROR;\n}\nOH_AI_DeviceInfoSetPerformanceMode(nnrt_device_info, OH_AI_PERFORMANCE_HIGH);\nOH_AI_ContextAddDeviceInfo(context, nnrt_device_info);\n\n// 其次设置CPU推理。\nOH_AI_DeviceInfoHandle cpu_device_info = OH_AI_DeviceInfoCreate(OH_AI_DEVICETYPE_CPU);\nif (cpu_device_info == NULL) {\n  printf(\"OH_AI_DeviceInfoCreate failed.\\n\");\n  OH_AI_ContextDestroy(&context);\n  return OH_AI_STATUS_LITE_ERROR;\n}\nOH_AI_ContextAddDeviceInfo(context, cpu_device_info); 创建、加载与编译模型。  调用OH_AI_ModelBuildFromFile加载并编译模型。  本例中传入OH_AI_ModelBuildFromFile的argv[1]参数是从控制台中输入的模型文件路径。 // 创建模型\nOH_AI_ModelHandle model = OH_AI_ModelCreate();\nif (model == NULL) {\n  printf(\"OH_AI_ModelCreate failed.\\n\");\n  OH_AI_ContextDestroy(&context);\n  return OH_AI_STATUS_LITE_ERROR;\n}\n\n// 加载与编译模型，模型的类型为OH_AI_MODELTYPE_MINDIR\nint ret = OH_AI_ModelBuildFromFile(model, argv[1], OH_AI_MODELTYPE_MINDIR, context);\nif (ret != OH_AI_STATUS_SUCCESS) {\n  printf(\"OH_AI_ModelBuildFromFile failed, ret: %d.\\n\", ret);\n  OH_AI_ModelDestroy(&model);\n  return ret;\n} 输入数据。  模型执行之前需要向输入的张量中填充数据。本例使用随机的数据对模型进行填充。 // 获得输入张量\nOH_AI_TensorHandleArray inputs = OH_AI_ModelGetInputs(model);\nif (inputs.handle_list == NULL) {\n  printf(\"OH_AI_ModelGetInputs failed, ret: %d.\\n\", ret);\n  OH_AI_ModelDestroy(&model);\n  return ret;\n}\n// 使用随机数据填充张量\nret = GenerateInputDataWithRandom(inputs);\nif (ret != OH_AI_STATUS_SUCCESS) {\n  printf(\"GenerateInputDataWithRandom failed, ret: %d.\\n\", ret);\n  OH_AI_ModelDestroy(&model);\n  return ret;\n} 执行推理。  使用OH_AI_ModelPredict接口进行模型推理。 // 执行模型推理\nOH_AI_TensorHandleArray outputs;\nret = OH_AI_ModelPredict(model, inputs, &outputs, NULL, NULL);\nif (ret != OH_AI_STATUS_SUCCESS) {\n  printf(\"OH_AI_ModelPredict failed, ret: %d.\\n\", ret);\n  OH_AI_ModelDestroy(&model);\n  return ret;\n} 获取输出。  模型推理结束之后，可以通过输出张量得到推理结果。 // 获取模型的输出张量，并打印\nfor (size_t i = 0; i < outputs.handle_num; ++i) {\n  OH_AI_TensorHandle tensor = outputs.handle_list[i];\n  int64_t element_num = OH_AI_TensorGetElementNum(tensor);\n  printf(\"Tensor name: %s, tensor size is %zu ,elements num: %lld.\\n\", OH_AI_TensorGetName(tensor),\n        OH_AI_TensorGetDataSize(tensor), element_num);\n  const float *data = (const float *)OH_AI_TensorGetData(tensor);\n  printf(\"output data is:\\n\");\n  const int max_print_num = 50;\n  for (int j = 0; j < element_num && j <= max_print_num; ++j) {\n    printf(\"%f \", data[j]);\n  }\n  printf(\"\\n\");\n} 释放模型。  不再使用MindSpore Lite推理框架时，需要释放已经创建的模型。 // 释放模型\nOH_AI_ModelDestroy(&model); 
调测验证
编写CMakeLists.txt。 cmake_minimum_required(VERSION 3.14)\nproject(Demo)\n\nadd_executable(demo main.c)\n\ntarget_link_libraries(\n        demo\n        mindspore_lite_ndk\n        pthread\n        dl\n) 使用ohos-sdk交叉编译，需要对CMake设置native工具链路径，即：-DCMAKE_TOOLCHAIN_FILE=\"/xxx/native/build/cmake/ohos.toolchain.cmake\"。 工具链默认编译64位的程序，如果要编译32位，需要添加：-DOHOS_ARCH=\"armeabi-v7a\"。  运行。 使用hdc_std连接设备，并将demo和mobilenetv2.ms推送到设备中的相同目录。使用hdc_std shell进入设备，并进入demo所在的目录执行如下命令，即可得到结果。 ./demo mobilenetv2.ms  得到如下输出: # ./QuickStart ./mobilenetv2.ms                                            \nTensor name: Softmax-65, tensor size is 4004 ,elements num: 1001.\noutput data is:\n0.000018 0.000012 0.000026 0.000194 0.000156 0.001501 0.000240 0.000825 0.000016 0.000006 0.000007 0.000004 0.000004 0.000004 0.000015 0.000099 0.000011 0.000013 0.000005 0.000023 0.000004 0.000008 0.000003 0.000003 0.000008 0.000014 0.000012 0.000006 0.000019 0.000006 0.000018 0.000024 0.000010 0.000002 0.000028 0.000372 0.000010 0.000017 0.000008 0.000004 0.000007 0.000010 0.000007 0.000012 0.000005 0.000015 0.000007 0.000040 0.000004 0.000085 0.000023  
