使用MindSpore Lite实现图像分类（C/C++）
场景说明
开发者可以使用MindSpore，在UI代码中直接集成MindSpore Lite能力，快速部署AI算法，进行AI模型推理，实现图像分类的应用。
图像分类可实现对图像中物体的识别，在医学影像分析、自动驾驶、电子商务、人脸识别等有广泛的应用。
基本概念
 N-API：用于构建ArkTS本地化组件的一套接口。可利用N-API，将C/C++开发的库封装成ArkTS模块。 
开发流程
 选择图像分类模型。 在端侧使用MindSpore Lite推理模型，实现对选择的图片进行分类。 
环境准备
安装DevEco Studio，要求版本 >= 4.1，并更新SDK到API 11或以上。
开发步骤
本文以对相册的一张图片进行推理为例，提供使用MindSpore Lite实现图像分类的开发指导。
[h2]选择模型
本示例程序中使用的图像分类模型文件为mobilenetv2.ms，放置在entry/src/main/resources/rawfile工程目录下。
如果开发者有其他图像分类的预训练模型，请参考MindSpore Lite 模型转换介绍，将原始模型转换成.ms格式。
[h2]编写代码
图像输入和预处理
 此处以获取相册图片为例，调用@ohos.file.picker 实现相册图片文件的选择。 import { photoAccessHelper } from '@kit.MediaLibraryKit';\nimport { BusinessError } from '@kit.BasicServicesKit';\n\nlet uris: Array<string> = [];\n\n// 创建图片文件选择实例\nlet photoSelectOptions = new photoAccessHelper.PhotoSelectOptions();\n\n// 设置选择媒体文件类型为IMAGE，设置选择媒体文件的最大数目\nphotoSelectOptions.MIMEType = photoAccessHelper.PhotoViewMIMETypes.IMAGE_TYPE;\nphotoSelectOptions.maxSelectNumber = 1;\n\n// 创建图库选择器实例，调用select()接口拉起图库界面进行文件选择。文件选择成功后，返回photoSelectResult结果集。\nlet photoPicker = new photoAccessHelper.PhotoViewPicker();\nphotoPicker.select(photoSelectOptions, async (\n  err: BusinessError, photoSelectResult: photoAccessHelper.PhotoSelectResult) => {\n  if (err) {\n    console.error('MS_LITE_ERR: PhotoViewPicker.select failed with err: ' + JSON.stringify(err));\n    return;\n  }\n  console.info('MS_LITE_LOG: PhotoViewPicker.select successfully, ' +\n    'photoSelectResult uri: ' + JSON.stringify(photoSelectResult));\n  uris = photoSelectResult.photoUris;\n  console.info('MS_LITE_LOG: uri: ' + uris);\n}) 根据模型的输入尺寸，调用@ohos.multimedia.image （实现图片处理）、@ohos.file.fs （实现基础文件操作） API对选择图片进行裁剪、获取图片buffer数据，并进行标准化处理。 import { image } from '@kit.ImageKit';\nimport { fileIo } from '@kit.CoreFileKit';\n\nlet modelInputHeight: number = 224;\nlet modelInputWidth: number = 224;\n\n// 使用fileIo.openSync接口，通过uri打开这个文件得到fd\nlet file = fileIo.openSync(this.uris[0], fileIo.OpenMode.READ_ONLY);\nconsole.info('MS_LITE_LOG: file fd: ' + file.fd);\n\n// 通过fd使用fileIo.readSync接口读取这个文件内的数据\nlet inputBuffer = new ArrayBuffer(4096000);\nlet readLen = fileIo.readSync(file.fd, inputBuffer);\nconsole.info('MS_LITE_LOG: readSync data to file succeed and inputBuffer size is:' + readLen);\n\n// 通过PixelMap预处理\nlet imageSource = image.createImageSource(file.fd);\nimageSource.createPixelMap().then((pixelMap) => {\n  pixelMap.getImageInfo().then((info) => {\n    console.info('MS_LITE_LOG: info.width = ' + info.size.width);\n    console.info('MS_LITE_LOG: info.height = ' + info.size.height);\n    // 根据模型输入的尺寸，将图片裁剪为对应的size，获取图片buffer数据readBuffer\n    pixelMap.scale(256.0 / info.size.width, 256.0 / info.size.height).then(() => {\n      pixelMap.crop(\n        { x: 16, y: 16, size: { height: modelInputHeight, width: modelInputWidth } }\n      ).then(async () => {\n        let info = await pixelMap.getImageInfo();\n        console.info('MS_LITE_LOG: crop info.width = ' + info.size.width);\n        console.info('MS_LITE_LOG: crop info.height = ' + info.size.height);\n        // 需要创建的像素buffer大小\n        let readBuffer = new ArrayBuffer(modelInputHeight * modelInputWidth * 4);\n        await pixelMap.readPixelsToBuffer(readBuffer);\n        console.info('MS_LITE_LOG: Succeeded in reading image pixel data, buffer: ' +\n        readBuffer.byteLength);\n        // 处理readBuffer，转换成float32格式，并进行标准化处理\n        const imageArr = new Uint8Array(\n          readBuffer.slice(0, modelInputHeight * modelInputWidth * 4));\n        console.info('MS_LITE_LOG: imageArr length: ' + imageArr.length);\n        let means = [0.485, 0.456, 0.406];\n        let stds = [0.229, 0.224, 0.225];\n        let float32View = new Float32Array(modelInputHeight * modelInputWidth * 3);\n        let index = 0;\n        for (let i = 0; i < imageArr.length; i++) {\n          if ((i + 1) % 4 == 0) {\n            float32View[index] = (imageArr[i - 3] / 255.0 - means[0]) / stds[0]; // B\n            float32View[index+1] = (imageArr[i - 2] / 255.0 - means[1]) / stds[1]; // G\n            float32View[index+2] = (imageArr[i - 1] / 255.0 - means[2]) / stds[2]; // R\n            index += 3;\n          }\n        }\n        console.info('MS_LITE_LOG: float32View length: ' + float32View.length);\n        let printStr = 'float32View data:';\n        for (let i = 0; i < 20; i++) {\n          printStr += ' ' + float32View[i];\n        }\n        console.info('MS_LITE_LOG: float32View data: ' + printStr);\n      })\n    })\n  });\n}); 
编写推理代码
调用MindSpore实现端侧推理，推理代码流程如下。
 引用对应的头文件 #include <iostream>\n#include <sstream>\n#include <stdlib.h>\n#include <hilog/log.h>\n#include <rawfile/raw_file_manager.h>\n#include <mindspore/types.h>\n#include <mindspore/model.h>\n#include <mindspore/context.h>\n#include <mindspore/status.h>\n#include <mindspore/tensor.h>\n#include \"napi/native_api.h\" 读取模型文件 #define LOGI(...) ((void)OH_LOG_Print(LOG_APP, LOG_INFO, LOG_DOMAIN, \"[MSLiteNapi]\", __VA_ARGS__))\n#define LOGD(...) ((void)OH_LOG_Print(LOG_APP, LOG_DEBUG, LOG_DOMAIN, \"[MSLiteNapi]\", __VA_ARGS__))\n#define LOGW(...) ((void)OH_LOG_Print(LOG_APP, LOG_WARN, LOG_DOMAIN, \"[MSLiteNapi]\", __VA_ARGS__))\n#define LOGE(...) ((void)OH_LOG_Print(LOG_APP, LOG_ERROR, LOG_DOMAIN, \"[MSLiteNapi]\", __VA_ARGS__))\n\nvoid *ReadModelFile(NativeResourceManager *nativeResourceManager, const std::string &modelName, size_t *modelSize) {\n    auto rawFile = OH_ResourceManager_OpenRawFile(nativeResourceManager, modelName.c_str());\n    if (rawFile == nullptr) {\n        LOGE(\"MS_LITE_ERR: Open model file failed\");\n        return nullptr;\n    }\n    long fileSize = OH_ResourceManager_GetRawFileSize(rawFile);\n    void *modelBuffer = malloc(fileSize);\n    if (modelBuffer == nullptr) {\n        LOGE(\"MS_LITE_ERR: OH_ResourceManager_ReadRawFile failed\");\n    }\n    int ret = OH_ResourceManager_ReadRawFile(rawFile, modelBuffer, fileSize);\n    if (ret == 0) {\n        LOGI(\"MS_LITE_LOG: OH_ResourceManager_ReadRawFile failed\");\n        OH_ResourceManager_CloseRawFile(rawFile);\n        return nullptr;\n    }\n    OH_ResourceManager_CloseRawFile(rawFile);\n    *modelSize = fileSize;\n    return modelBuffer;\n} 创建上下文，设置线程数、设备类型等参数，并加载模型。 void DestroyModelBuffer(void **buffer) {\n    if (buffer == nullptr) {\n        return;\n    }\n    free(*buffer);\n    *buffer = nullptr;\n}\n\nOH_AI_ModelHandle CreateMSLiteModel(void *modelBuffer, size_t modelSize) {\n    // Set executing context for model.\n    auto context = OH_AI_ContextCreate();\n    if (context == nullptr) {\n        DestroyModelBuffer(&modelBuffer);\n        LOGE(\"MS_LITE_ERR: Create MSLite context failed.\\n\");\n        return nullptr;\n    }\n    auto cpu_device_info = OH_AI_DeviceInfoCreate(OH_AI_DEVICETYPE_CPU);\n\n    OH_AI_DeviceInfoSetEnableFP16(cpu_device_info, true);\n    OH_AI_ContextAddDeviceInfo(context, cpu_device_info);\n\n    // Create model\n    auto model = OH_AI_ModelCreate();\n    if (model == nullptr) {\n        DestroyModelBuffer(&modelBuffer);\n        LOGE(\"MS_LITE_ERR: Allocate MSLite Model failed.\\n\");\n        return nullptr;\n    }\n\n    // Build model object\n    auto build_ret = OH_AI_ModelBuild(model, modelBuffer, modelSize, OH_AI_MODELTYPE_MINDIR, context);\n    DestroyModelBuffer(&modelBuffer);\n    if (build_ret != OH_AI_STATUS_SUCCESS) {\n        OH_AI_ModelDestroy(&model);\n        LOGE(\"MS_LITE_ERR: Build MSLite model failed.\\n\");\n        return nullptr;\n    }\n    LOGI(\"MS_LITE_LOG: Build MSLite model success.\\n\");\n    return model;\n} 设置模型输入数据，执行模型推理。 constexpr int K_NUM_PRINT_OF_OUT_DATA = 20;\n\n// 设置模型输入数据\nint FillInputTensor(OH_AI_TensorHandle input, std::vector<float> input_data) {\n    if (OH_AI_TensorGetDataType(input) == OH_AI_DATATYPE_NUMBERTYPE_FLOAT32) {\n        float *data = (float *)OH_AI_TensorGetMutableData(input);\n        for (size_t i = 0; i < OH_AI_TensorGetElementNum(input); i++) {\n            data[i] = input_data[i];\n        }\n        return OH_AI_STATUS_SUCCESS;\n    } else {\n        return OH_AI_STATUS_LITE_ERROR;\n    }\n}\n\n// 执行模型推理\nint RunMSLiteModel(OH_AI_ModelHandle model, std::vector<float> input_data) {\n    // Set input data for model.\n    auto inputs = OH_AI_ModelGetInputs(model);\n\n    auto ret = FillInputTensor(inputs.handle_list[0], input_data);\n    if (ret != OH_AI_STATUS_SUCCESS) {\n        LOGE(\"MS_LITE_ERR: RunMSLiteModel set input error.\\n\");\n        return OH_AI_STATUS_LITE_ERROR;\n    }\n    // Get model output.\n    auto outputs = OH_AI_ModelGetOutputs(model);\n    // Predict model.\n    auto predict_ret = OH_AI_ModelPredict(model, inputs, &outputs, nullptr, nullptr);\n    if (predict_ret != OH_AI_STATUS_SUCCESS) {\n        OH_AI_ModelDestroy(&model);\n        LOGE(\"MS_LITE_ERR: MSLite Predict error.\\n\");\n        return OH_AI_STATUS_LITE_ERROR;\n    }\n    LOGI(\"MS_LITE_LOG: Run MSLite model Predict success.\\n\");\n    // Print output tensor data.\n    LOGI(\"MS_LITE_LOG: Get model outputs:\\n\");\n    for (size_t i = 0; i < outputs.handle_num; i++) {\n        auto tensor = outputs.handle_list[i];\n        LOGI(\"MS_LITE_LOG: - Tensor %{public}d name is: %{public}s.\\n\", static_cast<int>(i),\n             OH_AI_TensorGetName(tensor));\n        LOGI(\"MS_LITE_LOG: - Tensor %{public}d size is: %{public}d.\\n\", static_cast<int>(i),\n             (int)OH_AI_TensorGetDataSize(tensor));\n        LOGI(\"MS_LITE_LOG: - Tensor data is:\\n\");\n        auto out_data = reinterpret_cast<const float *>(OH_AI_TensorGetData(tensor));\n        std::stringstream outStr;\n        for (int i = 0; (i < OH_AI_TensorGetElementNum(tensor)) && (i <= K_NUM_PRINT_OF_OUT_DATA); i++) {\n            outStr << out_data[i] << \" \";\n        }\n        LOGI(\"MS_LITE_LOG: %{public}s\", outStr.str().c_str());\n    }\n    return OH_AI_STATUS_SUCCESS;\n} 调用以上方法，实现完整的模型推理流程。 static napi_value RunDemo(napi_env env, napi_callback_info info) {\n    LOGI(\"MS_LITE_LOG: Enter runDemo()\");\n    napi_value error_ret;\n    napi_create_int32(env, -1, &error_ret);\n    // 传入数据处理\n    size_t argc = 2;\n    napi_value argv[2] = {nullptr};\n    napi_get_cb_info(env, info, &argc, argv, nullptr, nullptr);\n    bool isArray = false;\n    napi_is_array(env, argv[0], &isArray);\n    uint32_t length = 0;\n    // 获取数组的长度\n    napi_get_array_length(env, argv[0], &length);\n    LOGI(\"MS_LITE_LOG: argv array length = %{public}d\", length);\n    std::vector<float> input_data;\n    double param = 0;\n    for (int i = 0; i < length; i++) {\n        napi_value value;\n        napi_get_element(env, argv[0], i, &value);\n        napi_get_value_double(env, value, &param);\n        input_data.push_back(static_cast<float>(param));\n    }\n    std::stringstream outstr;\n    for (int i = 0; i < K_NUM_PRINT_OF_OUT_DATA; i++) {\n        outstr << input_data[i] << \" \";\n    }\n    LOGI(\"MS_LITE_LOG: input_data = %{public}s\", outstr.str().c_str());\n    // Read model file\n    const std::string modelName = \"mobilenetv2.ms\";\n    LOGI(\"MS_LITE_LOG: Run model: %{public}s\", modelName.c_str());\n    size_t modelSize;\n    auto resourcesManager = OH_ResourceManager_InitNativeResourceManager(env, argv[1]);\n    auto modelBuffer = ReadModelFile(resourcesManager, modelName, &modelSize);\n    if (modelBuffer == nullptr) {\n        LOGE(\"MS_LITE_ERR: Read model failed\");\n        return error_ret;\n    }\n    LOGI(\"MS_LITE_LOG: Read model file success\");\n    auto model = CreateMSLiteModel(modelBuffer, modelSize);\n    if (model == nullptr) {\n        OH_AI_ModelDestroy(&model);\n        LOGE(\"MS_LITE_ERR: MSLiteFwk Build model failed.\\n\");\n        return error_ret;\n    }\n    int ret = RunMSLiteModel(model, input_data);\n    if (ret != OH_AI_STATUS_SUCCESS) {\n        OH_AI_ModelDestroy(&model);\n        LOGE(\"MS_LITE_ERR: RunMSLiteModel failed.\\n\");\n        return error_ret;\n    }\n    napi_value out_data;\n    napi_create_array(env, &out_data);\n    auto outputs = OH_AI_ModelGetOutputs(model);\n    OH_AI_TensorHandle output_0 = outputs.handle_list[0];\n    float *output0Data = reinterpret_cast<float *>(OH_AI_TensorGetMutableData(output_0));\n    for (size_t i = 0; i < OH_AI_TensorGetElementNum(output_0); i++) {\n        napi_value element;\n        napi_create_double(env, static_cast<double>(output0Data[i]), &element);\n        napi_set_element(env, out_data, i, element);\n    }\n    OH_AI_ModelDestroy(&model);\n    LOGI(\"MS_LITE_LOG: Exit runDemo()\");\n    return out_data;\n} 编写CMake脚本，链接MindSpore Lite动态库。 # the minimum version of CMake.\ncmake_minimum_required(VERSION 3.4.1)\nproject(MindSporeLiteCDemo)\n\nset(NATIVERENDER_ROOT_PATH ${CMAKE_CURRENT_SOURCE_DIR})\n\nif(DEFINED PACKAGE_FIND_FILE)\n    include(${PACKAGE_FIND_FILE})\nendif()\n\ninclude_directories(${NATIVERENDER_ROOT_PATH}\n                    ${NATIVERENDER_ROOT_PATH}/include)\n\nadd_library(entry SHARED mslite_napi.cpp)\ntarget_link_libraries(entry PUBLIC mindspore_lite_ndk)\ntarget_link_libraries(entry PUBLIC hilog_ndk.z)\ntarget_link_libraries(entry PUBLIC rawfile.z)\ntarget_link_libraries(entry PUBLIC ace_napi.z) 
使用N-API将C++动态库封装成ArkTS模块
 在 entry/src/main/cpp/types/libentry/Index.d.ts，定义ArkTS接口runDemo() 。内容如下： export const runDemo: (a: number[], b:Object) => Array<number>; 在 oh-package.json5 文件，将API与so相关联，成为一个完整的ArkTS模块： {\n  \"name\": \"libentry.so\",\n  \"types\": \"./Index.d.ts\",\n  \"version\": \"1.0.0\",\n  \"description\": \"MindSpore Lite inference module\"\n} 
调用封装的ArkTS模块进行推理并输出结果
在 entry/src/main/ets/pages/Index.ets 中，调用封装的ArkTS模块，最后对推理结果进行处理。
import msliteNapi from 'libentry.so'\nimport { resourceManager } from '@kit.LocalizationKit';\n\nlet resMgr: resourceManager.ResourceManager = getContext().getApplicationContext().resourceManager;\nlet max: number = 0;\nlet maxIndex: number = 0;\nlet maxArray: Array<number> = [];\nlet maxIndexArray: Array<number> = [];\n\n// 调用c++的runDemo方法，完成图像输入和预处理后的buffer数据保存在float32View，具体可见上文图像输入和预处理中float32View的定义和处理。\nconsole.info('MS_LITE_LOG: *** Start MSLite Demo ***');\nlet output: Array<number> = msliteNapi.runDemo(Array.from(float32View), resMgr);\n// 取分类占比的最大值\nthis.max = 0;\nthis.maxIndex = 0;\nthis.maxArray = [];\nthis.maxIndexArray = [];\nlet newArray = output.filter(value => value !== max);\nfor (let n = 0; n < 5; n++) {\n  max = output[0];\n  maxIndex = 0;\n  for (let m = 0; m < newArray.length; m++) {\n    if (newArray[m] > max) {\n      max = newArray[m];\n      maxIndex = m;\n    }\n  }\n  maxArray.push(Math.round(this.max * 10000));\n  maxIndexArray.push(this.maxIndex);\n  // filter函数数组过滤函数\n  newArray = newArray.filter(value => value !== max);\n}\nconsole.info('MS_LITE_LOG: max:' + this.maxArray);\nconsole.info('MS_LITE_LOG: maxIndex:' + this.maxIndexArray);\nconsole.info('MS_LITE_LOG: *** Finished MSLite Demo ***');
[h2]调测验证
 在DevEco Studio中连接设备，点击Run entry，编译Hap，有如下显示： Launching com.samples.mindsporelitecdemo\n$ hdc shell aa force-stop com.samples.mindsporelitecdemo\n$ hdc shell mkdir data/local/tmp/xxx\n$ hdc file send C:\\Users\\xxx\\MindSporeLiteCDemo\\entry\\build\\default\\outputs\\default\\entry-default-signed.hap \"data/local/tmp/xxx\"\n$ hdc shell bm install -p data/local/tmp/xxx\n$ hdc shell rm -rf data/local/tmp/xxx\n$ hdc shell aa start -a EntryAbility -b com.samples.mindsporelitecdemo 在设备屏幕点击photo按钮，选择图片，点击确定。设备屏幕显示所选图片的分类结果，在日志打印结果中，过滤关键字”MS_LITE“，可得到如下结果： 08-05 17:15:52.001   4684-4684    A03d00/JSAPP                   pid-4684              I     MS_LITE_LOG: PhotoViewPicker.select successfully, photoSelectResult uri: {\"photoUris\":[\"file://media/Photo/13/IMG_1501955351_012/plant.jpg\"]}\n...\n08-05 17:15:52.627   4684-4684    A03d00/JSAPP                   pid-4684              I     MS_LITE_LOG: crop info.width = 224\n08-05 17:15:52.627   4684-4684    A03d00/JSAPP                   pid-4684              I     MS_LITE_LOG: crop info.height = 224\n08-05 17:15:52.628   4684-4684    A03d00/JSAPP                   pid-4684              I     MS_LITE_LOG: Succeeded in reading image pixel data, buffer: 200704\n08-05 17:15:52.971   4684-4684    A03d00/JSAPP                   pid-4684              I     MS_LITE_LOG: float32View data: float32View data: 1.2385478019714355 1.308123230934143 1.4722440242767334 1.2385478019714355 1.308123230934143 1.4722440242767334 1.2385478019714355 1.308123230934143 1.4722440242767334 1.2385478019714355 1.308123230934143 1.4722440242767334 1.2385478019714355 1.308123230934143 1.4722440242767334 1.2385478019714355 1.308123230934143 1.4722440242767334 1.2385478019714355 1.308123230934143\n08-05 17:15:52.971   4684-4684    A03d00/JSAPP                   pid-4684              I     MS_LITE_LOG: *** Start MSLite Demo ***\n08-05 17:15:53.454   4684-4684    A00000/[MSLiteNapi]            pid-4684              I     MS_LITE_LOG: Build MSLite model success.\n08-05 17:15:53.753   4684-4684    A00000/[MSLiteNapi]            pid-4684              I     MS_LITE_LOG: Run MSLite model Predict success.\n08-05 17:15:53.753   4684-4684    A00000/[MSLiteNapi]            pid-4684              I     MS_LITE_LOG: Get model outputs:\n08-05 17:15:53.753   4684-4684    A00000/[MSLiteNapi]            pid-4684              I     MS_LITE_LOG: - Tensor 0 name is: Default/head-MobileNetV2Head/Sigmoid-op466.\n08-05 17:15:53.753   4684-4684    A00000/[MSLiteNapi]            pid-4684              I     MS_LITE_LOG: - Tensor data is:\n08-05 17:15:53.753   4684-4684    A00000/[MSLiteNapi]            pid-4684              I     MS_LITE_LOG: 3.43385e-06 1.40285e-05 9.11969e-07 4.91007e-05 9.50266e-07 3.94537e-07 0.0434676 3.97196e-05 0.00054832 0.000246202 1.576e-05 3.6494e-06 1.23553e-05 0.196977 5.3028e-05 3.29346e-05 4.90475e-07 1.66109e-06 7.03273e-06 8.83677e-07 3.1365e-06\n08-05 17:15:53.781   4684-4684    A03d00/JSAPP                   pid-4684              W     MS_LITE_WARN: output length =  500 ;value =  0.0000034338463592575863,0.000014028532859811094,9.119685273617506e-7,0.000049100715841632336,9.502661555416125e-7,3.945370394831116e-7,0.04346757382154465,0.00003971960904891603,0.0005483203567564487,0.00024620210751891136,0.000015759984307806008,0.0000036493988773145247,0.00001235533181898063,0.1969769448041916,0.000053027983085485175,0.000032934600312728435,4.904751449430478e-7,0.0000016610861166554969,0.000007032729172351537,8.836767619868624e-7\n08-05 17:15:53.831   4684-4684    A03d00/JSAPP                   pid-4684              I     MS_LITE_LOG: max:9497,7756,1970,435,46\n08-05 17:15:53.831   4684-4684    A03d00/JSAPP                   pid-4684              I     MS_LITE_LOG: maxIndex:323,46,13,6,349\n08-05 17:15:53.831   4684-4684    A03d00/JSAPP                   pid-4684              I     MS_LITE_LOG: *** Finished MSLite Demo *** 
[h2]效果示意
在设备上，点击photo按钮，选择相册中的一张图片，点击确定。在图片下方显示此图片占比前4的分类信息。
