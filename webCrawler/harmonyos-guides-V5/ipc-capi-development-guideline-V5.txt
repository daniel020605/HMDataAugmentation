IPC通信开发指导（C/C++）
场景介绍
IPC的主要工作是让运行在不同进程的Proxy和Stub互相通信，而IPC CAPI是提供的C接口。
IPC CAPI接口不直接提供跨进程通信能力，两个进程之间的IPC通道建立，依赖于Ability Kit。
进程间IPC通道建立，详情参考Native子进程开发指导（C/C++)，本文重点阐述IPC CAPI部分使用说明。
接口说明
表1 CAPI侧IPC接口
typedef int (*OH_OnRemoteRequestCallback)
(uint32_t code, const OHIPCParcel *data, OHIPCParcel *reply,
void *userData);
OHIPCRemoteStub* OH_IPCRemoteStub_Create
(const char *descriptor, OH_OnRemoteRequestCallback requestCallback,
OH_OnRemoteDestroyCallback destroyCallback, void *userData);
int OH_IPCRemoteProxy_SendRequest(const OHIPCRemoteProxy *proxy,
uint32_t code, const OHIPCParcel *data, OHIPCParcel *reply,
const OH_IPC_MessageOption *option);
OHIPCRemoteProxy对象，用于向远端发送请求。
需要依赖元能力接口返回。
OHIPCDeathRecipient* OH_IPCDeathRecipient_Create
(OH_OnDeathRecipientCallback deathRecipientCallback,
OH_OnDeathRecipientDestroyCallback destroyCallback,
void *userData);
int OH_IPCRemoteProxy_AddDeathRecipient(OHIPCRemoteProxy *proxy,
OHIPCDeathRecipient *recipient);
向OHIPCRemoteProxy对象添加死亡监听，
用于接收远端OHIPCRemoteStub对象死亡的回调通知。
详细的接口说明请参考IPCKit。
开发步骤
以下步骤描述了如何使用IPCKit提供的CAPI接口，创建远端Stub和使用客户端代理Proxy进行通信，同时兼备远端死亡通知接收能力。
[h2]1. 添加动态链接库
CMakeLists.txt中添加以下lib。
# ipc capi\nlibipc_capi.so\n# 元能力，ability capi\nlibchild_process.so
[h2]2. 头文件
// ipc capi\n#include <IPCKit/ipc_kit.h>\n// 元能力，ability capi\n#include <AbilityKit/native_child_process.h>
[h2]3. 异步调用场景
3.1 公共数据及函数定义
#include <string>\n#include <thread>\n#include <mutex>\n#include <chrono>\n#include <condition_variable>\n#include <IPCKit/ipc_kit.h>\n#include <AbilityKit/native_child_process.h>\n#include <hilog/log.h>\n#undef LOG_DOMAIN\n#undef LOG_TAG\n#define LOG_DOMAIN 0x0201\n#define LOG_TAG \"IPCCApiSample\"\n\nenum RequestCode {\n    ASYNC_ADD_CODE = 1,\n    REQUEST_EXIT_CODE = 2,\n    OTHER_CODE\n};\nstatic constexpr int MAX_MEMORY_SIZE = 204800;\nstatic const std::string INTERFACE_DESCRIPTOR = \"INTERFACE_DESCRIPTOR\";\nstatic const std::string NATIVE_REMOTE_STUB_TEST_TOKEN = \"native.remote.stub\";\nstatic const std::string NATIVE_REMOTE_STUB_ASYNC_CALL_TEST_TOKEN = \"native.remote.stub.async.call\";\n\n// 定义内存分配函数\nstatic void* LocalMemoryAllocator(int32_t len) {\n    if (len < 0 || len > MAX_MEMORY_SIZE ) {\n        return nullptr;\n    }\n    void *buffer = malloc(len);\n    if (buffer == nullptr) {\n        return nullptr;\n    }\n    memset(buffer, 0, len);\n    return buffer;\n}
3.2 服务端对象: IpcCApiStubTest
class IpcCApiStubTest {\npublic:\n    explicit IpcCApiStubTest();\n    ~IpcCApiStubTest();\n    void MainProc();\n    OHIPCRemoteStub* GetRemoteStub();\n    static int OnRemoteRequest(uint32_t code, const OHIPCParcel *data, OHIPCParcel *reply, void *userData);\nprivate:\n    int AsyncAdd(const OHIPCParcel *data);\n    int RequestExitChildProcess();\nprivate:\n    OHIPCRemoteStub *stub_{ nullptr };\n    std::mutex childMutex_;\n    std::condition_variable childCondVar_;\n};\n\nIpcCApiStubTest::IpcCApiStubTest() {\n    stub_ = OH_IPCRemoteStub_Create(INTERFACE_DESCRIPTOR.c_str(), &IpcCApiStubTest::OnRemoteRequest,\n        nullptr, this);\n}\n\nIpcCApiStubTest::~IpcCApiStubTest() {\n    if (stub_ != nullptr) {\n        OH_IPCRemoteStub_Destroy(stub_);\n    }\n}\n\nvoid IpcCApiStubTest::MainProc() {\n    std::unique_lock<std::mutex> autoLock(childMutex_);\n    childCondVar_.wait(autoLock);\n}\n\nOHIPCRemoteStub* IpcCApiStubTest::GetRemoteStub() {\n    return stub_;\n}\n\nint IpcCApiStubTest::OnRemoteRequest(uint32_t code, const OHIPCParcel *data, OHIPCParcel *reply, void *userData) {\n    int readLen = 0;\n    char *token = nullptr;\n    // 接口校验\n    if (OH_IPCParcel_ReadInterfaceToken(data, &token, &readLen, LocalMemoryAllocator) != OH_IPC_SUCCESS\n        || NATIVE_REMOTE_STUB_TEST_TOKEN != token) {\n        if (token != nullptr) {\n            OH_LOG_ERROR(LOG_APP, \"check InterfaceToken failed\");\n            free(token);\n        }\n        return OH_IPC_PARCEL_WRITE_ERROR;\n    }\n    free(token);\n    auto *stubTest = reinterpret_cast<IpcCApiStubTest *>(userData);\n    if (stubTest == nullptr) {\n        return OH_IPC_CHECK_PARAM_ERROR;\n    }\n    auto rqCode = RequestCode(code);\n    switch (rqCode) {\n        case ASYNC_ADD_CODE: {\n            return stubTest->AsyncAdd(data);\n        }\n        case REQUEST_EXIT_CODE: {\n            return stubTest->RequestExitChildProcess();\n        }\n        default:\n            break;\n    }\n    return OH_IPC_SUCCESS;\n}\n\nint IpcCApiStubTest::AsyncAdd(const OHIPCParcel *data) {\n    int a = 0;\n    int b = 0;\n    OH_LOG_INFO(LOG_APP, \"start async add a=%d,b=%d\", a, b);\n    if ((OH_IPCParcel_ReadInt32(data, &a) != OH_IPC_SUCCESS)\n        || (OH_IPCParcel_ReadInt32(data, &b) != OH_IPC_SUCCESS)) {\n        return OH_IPC_PARCEL_READ_ERROR;\n    }\n    auto proxyCallBack = OH_IPCParcel_ReadRemoteProxy(data);\n    if (proxyCallBack == nullptr) {\n        return OH_IPC_PARCEL_READ_ERROR;\n    }\n    OH_LOG_INFO(LOG_APP, \"start create sendCallBack thread!\");\n    // 此处开启线程异步完成功能实现并利用proxyCallBack完成结果响应，如果同步调用，则直接通过replyData写入响应结果即可\n    std::thread th([proxyCallBack, a, b] {\n        auto data = OH_IPCParcel_Create();\n        if (data == nullptr) {\n            OH_IPCRemoteProxy_Destroy(proxyCallBack);\n            return;\n        }\n        auto reply = OH_IPCParcel_Create();\n        if (reply == nullptr) {\n            OH_IPCParcel_Destroy(data);\n            OH_IPCRemoteProxy_Destroy(proxyCallBack);\n            return;\n        }\n        if (OH_IPCParcel_WriteInt32(data, a + b) != OH_IPC_SUCCESS) {\n            OH_IPCParcel_Destroy(data);\n            OH_IPCParcel_Destroy(reply);\n            OH_IPCRemoteProxy_Destroy(proxyCallBack);\n            return;\n        }\n        // 异步线程处理结果通过IPC同步调用方式返回给业务请求方\n        OH_IPC_MessageOption option = { OH_IPC_REQUEST_MODE_SYNC, 0 };\n        OH_LOG_INFO(LOG_APP, \"thread start sendCallBack!\");\n        int ret = OH_IPCRemoteProxy_SendRequest(proxyCallBack, ASYNC_ADD_CODE, data, reply, &option);\n        OH_LOG_INFO(LOG_APP, \"thread sendCallBack ret = %d\", ret);\n        if (ret != OH_IPC_SUCCESS) {\n            OH_IPCParcel_Destroy(data);\n            OH_IPCParcel_Destroy(reply);\n            OH_IPCRemoteProxy_Destroy(proxyCallBack);\n            return;\n        }\n        OH_IPCRemoteProxy_Destroy(proxyCallBack);\n        OH_IPCParcel_Destroy(data);\n        OH_IPCParcel_Destroy(reply);\n    });\n    th.detach();\n    return OH_IPC_SUCCESS;\n}\n\nint IpcCApiStubTest::RequestExitChildProcess() {\n    std::unique_lock<std::mutex> autoLock(childMutex_);\n    childCondVar_.notify_all();\n    return OH_IPC_SUCCESS;\n}
3.3 客户端代理对象: IpcCApiProxyTest
// 用戶自定义错误码\nstatic constexpr int OH_IPC_CREATE_OBJECT_ERROR = OH_IPC_USER_ERROR_CODE_MIN + 1;\n\nclass IpcCApiProxyTest {\npublic:\n    explicit IpcCApiProxyTest(OHIPCRemoteProxy *proxy);\n    ~IpcCApiProxyTest();\npublic:\n    int AsyncAdd(int a, int b, int &result);\n    int RequestExitChildProcess();\n    void ClearResource();\nprivate:\n    void SendAsyncReply(int &replyValue);\n    int WaitForAsyncReply(int timeOut);\n    static int OnRemoteRequest(uint32_t code, const OHIPCParcel *data,\n        OHIPCParcel *reply, void *userData);\n    static void OnDeathRecipientCB(void *userData);\nprivate:\n    int asyncReply_{};\n    std::mutex mutex_;\n    std::condition_variable cv_;\n    OHIPCRemoteProxy *proxy_{ nullptr };\n    OHIPCRemoteStub *replyStub_{ nullptr };\n    OHIPCDeathRecipient *deathRecipient_{ nullptr };\n};\n\nIpcCApiProxyTest::IpcCApiProxyTest(OHIPCRemoteProxy *proxy) {\n    if (proxy == nullptr) {\n        OH_LOG_ERROR(LOG_APP, \"proxy is nullptr\");\n        return;\n    }\n    proxy_ = proxy;\n    replyStub_ = OH_IPCRemoteStub_Create(NATIVE_REMOTE_STUB_ASYNC_CALL_TEST_TOKEN.c_str(), OnRemoteRequest,\n        nullptr, this);\n    if (replyStub_ == nullptr) {\n        OH_LOG_ERROR(LOG_APP, \"crete reply stub failed!\");\n        return;\n    }\n    deathRecipient_ = OH_IPCDeathRecipient_Create(OnDeathRecipientCB, nullptr, this);\n    if (deathRecipient_ == nullptr) {\n        OH_LOG_ERROR(LOG_APP, \"OH_IPCDeathRecipient_Create failed!\");\n        return;\n    }\n    OH_IPCRemoteProxy_AddDeathRecipient(proxy_, deathRecipient_);\n}\n\nIpcCApiProxyTest::~IpcCApiProxyTest() {\n    if (proxy_ != nullptr) {\n        OH_IPCRemoteProxy_Destroy(proxy_);\n    }\n    if (deathRecipient_ != nullptr) {\n        OH_IPCDeathRecipient_Destroy(deathRecipient_);\n    }\n    if (replyStub_ != nullptr) {\n        OH_IPCRemoteStub_Destroy(replyStub_);\n    }\n}\n\nint IpcCApiProxyTest::AsyncAdd(int a, int b, int &result) {\n    OH_LOG_INFO(LOG_APP, \"start %d + %d\", a, b);\n    auto data = OH_IPCParcel_Create();\n    if (data == nullptr) {\n        return OH_IPC_CREATE_OBJECT_ERROR;\n    }\n    // 写入接口校验token\n    if (OH_IPCParcel_WriteInterfaceToken(data, NATIVE_REMOTE_STUB_TEST_TOKEN.c_str()) != OH_IPC_SUCCESS) {\n        OH_LOG_ERROR(LOG_APP, \"OH_IPCParcel_WriteInterfaceToken failed!\");\n        OH_IPCParcel_Destroy(data);\n        return OH_IPC_PARCEL_WRITE_ERROR;\n    }\n    if (OH_IPCParcel_WriteInt32(data, a) != OH_IPC_SUCCESS\n        || OH_IPCParcel_WriteInt32(data, b) != OH_IPC_SUCCESS\n        || OH_IPCParcel_WriteRemoteStub(data, replyStub_) != OH_IPC_SUCCESS) {\n        OH_IPCParcel_Destroy(data);\n        return OH_IPC_PARCEL_WRITE_ERROR;\n    }\n    // 异步发送使用replyStub_进行响应结果接收，异步处理需要写入用于接收结果的OHIPCRemoteStub对象\n    OH_IPC_MessageOption option = { OH_IPC_REQUEST_MODE_ASYNC, 0 };\n    int ret = OH_IPCRemoteProxy_SendRequest(proxy_, RequestCode::ASYNC_ADD_CODE, data, nullptr, &option);\n    if (ret != OH_IPC_SUCCESS) {\n        OH_IPCParcel_Destroy(data);\n        OH_LOG_ERROR(LOG_APP, \"OH_IPCRemoteProxy_SendRequest failed!\");\n        return ret;\n    }\n    static constexpr int TIMEOUT = 3;\n    WaitForAsyncReply(TIMEOUT);\n    OH_LOG_INFO(LOG_APP, \"asyncReply_:%d\", asyncReply_);\n    result = asyncReply_;\n    OH_IPCParcel_Destroy(data);\n    return OH_IPC_SUCCESS;\n}\n\nint IpcCApiProxyTest::RequestExitChildProcess() {\n    auto data = OH_IPCParcel_Create();\n    if (data == nullptr) {\n        return OH_IPC_CREATE_OBJECT_ERROR;\n    }\n    auto reply = OH_IPCParcel_Create();\n    if (reply == nullptr) {\n        OH_IPCParcel_Destroy(data);\n        return OH_IPC_CREATE_OBJECT_ERROR;\n    }\n    if (OH_IPCParcel_WriteInterfaceToken(data, NATIVE_REMOTE_STUB_TEST_TOKEN.c_str()) != OH_IPC_SUCCESS) {\n        OH_LOG_ERROR(LOG_APP, \"OH_IPCParcel_WriteInterfaceToken failed!\");\n        OH_IPCParcel_Destroy(data);\n        OH_IPCParcel_Destroy(reply);\n        return OH_IPC_PARCEL_WRITE_ERROR;\n    }\n    OH_IPC_MessageOption option = { OH_IPC_REQUEST_MODE_SYNC, 0 };\n    int ret = OH_IPCRemoteProxy_SendRequest(proxy_, RequestCode::REQUEST_EXIT_CODE, data, reply, &option);\n    if (ret != OH_IPC_SUCCESS) {\n        OH_IPCParcel_Destroy(data);\n        OH_IPCParcel_Destroy(reply);\n        OH_LOG_ERROR(LOG_APP, \"OH_IPCRemoteProxy_SendRequest failed!\");\n        return ret;\n    }\n    OH_IPCParcel_Destroy(data);\n    OH_IPCParcel_Destroy(reply);\n    return OH_IPC_SUCCESS;\n}\n\nvoid IpcCApiProxyTest::SendAsyncReply(int &replyValue) {\n    std::unique_lock<std::mutex> lck(mutex_);\n    asyncReply_ = replyValue;\n    cv_.notify_all();\n}\n\nint IpcCApiProxyTest::WaitForAsyncReply(int timeOut) {\n    asyncReply_ = 0;\n    std::unique_lock<std::mutex> lck(mutex_);\n    cv_.wait_for(lck, std::chrono::seconds(timeOut), [&] {\n        return asyncReply_ != 0;\n    });\n    return asyncReply_;\n}\n\nint IpcCApiProxyTest::OnRemoteRequest(uint32_t code, const OHIPCParcel *data,\n        OHIPCParcel *reply, void *userData) {\n    OH_LOG_INFO(LOG_APP, \"start %u\", code);\n    auto *proxyTest = reinterpret_cast<IpcCApiProxyTest *>(userData);\n    if (proxyTest == nullptr || code != static_cast<uint32_t>(RequestCode::ASYNC_ADD_CODE)) {\n        OH_LOG_ERROR(LOG_APP, \"check param failed!\");\n        return OH_IPC_CHECK_PARAM_ERROR;\n    }\n    int32_t val = -1;\n    if (OH_IPCParcel_ReadInt32(data, &val) != OH_IPC_SUCCESS) {\n        OH_LOG_ERROR(LOG_APP, \"OH_IPCParcel_ReadInt32 failed!\");\n        return OH_IPC_PARCEL_READ_ERROR;\n    }\n    proxyTest->SendAsyncReply(val);\n    return OH_IPC_SUCCESS;\n}\n\nvoid IpcCApiProxyTest::ClearResource() {\n    // clear resource;\n}\n\nvoid IpcCApiProxyTest::OnDeathRecipientCB(void *userData) {\n    auto *proxyTest = reinterpret_cast<IpcCApiProxyTest *>(userData);\n    if (proxyTest != nullptr) {\n        proxyTest->ClearResource();\n    }\n    OH_LOG_INFO(LOG_APP, \"the stub is dead!\");\n}
3.4 服务端调用入口，服务端文件\"libipcCapiDemo.so\"
IpcCApiStubTest g_ipcStubObj;\n\n#ifdef __cplusplus\nextern \"C\" {\n\n// 服务需要实现如下函数，具体可参考元能力接口说明\nOHIPCRemoteStub* NativeChildProcess_OnConnect() {\n    OH_LOG_INFO(LOG_APP, \"NativeChildProcess_OnConnect\");\n    return g_ipcStubObj.GetRemoteStub();\n}\n\nvoid NativeChildProcess_MainProc() {\n    OH_LOG_INFO(LOG_APP, \"NativeChildProcess_MainProc\");\n    g_ipcStubObj.MainProc();\n    OH_LOG_INFO(LOG_APP, \"NativeChildProcess_MainProc End\");\n}\n\n}\n#endif
3.5 客户端调用入口
IpcCApiProxyTest *g_ipcProxy = nullptr;\n\n// 元能力打通IPC通道回调接口\nvoid OnNativeChildProcessStarted(int errCode, OHIPCRemoteProxy *remoteProxy) {\n    OH_LOG_INFO(LOG_APP, \"OnNativeChildProcessStarted proxy=%{public}p err=%{public}d\", remoteProxy, errCode);\n    if (remoteProxy == nullptr) {\n        return;\n    }\n    g_ipcProxy = new (std::nothrow) IpcCApiProxyTest(remoteProxy);\n    if (g_ipcProxy == nullptr) {\n        OH_IPCRemoteProxy_Destroy(remoteProxy);\n        OH_LOG_ERROR(LOG_APP, \"Alloc IpcCApiProxyTest object failed\");\n        return;\n    }\n}\n\nint main(int argc, char *argv[]) {\n    int32_t ret = OH_Ability_CreateNativeChildProcess(\"libipcCapiDemo.so\", OnNativeChildProcessStarted);\n    if (ret != 0) {\n        return -1;        \n    }\n    if (g_ipcProxy == nullptr) {\n        return -1;        \n    }\n    int a = 2;\n    int b = 3;\n    int result = 0;    \n    ret = g_ipcProxy->AsyncAdd(a, b, result);\n    OH_LOG_INFO(LOG_APP, \"AsyncAdd: %d + %d = %d, ret=%d\", a, b, result, ret);\n\n    //kill stub端\n    ret = g_ipcProxy->RequestExitChildProcess();\n    //控制台输出： the stub is dead!\n    if (g_ipcProxy != nullptr) {\n        delete g_ipcProxy;\n        g_ipcProxy = nullptr;\n    }\n    return 0;\n}
