使用MindSpore Lite引擎进行端侧训练 (C/C++)
场景介绍
MindSpore Lite是一款AI引擎，它提供了面向不同硬件设备AI模型推理的功能，目前已经在图像分类、目标识别、人脸识别、文字识别等应用中广泛使用，同时支持在端侧设备上进行部署训练，让模型在实际业务场景中自适应用户的行为。
本文介绍使用MindSpore Lite端侧AI引擎进行模型训练的通用开发流程。
接口说明
此处给出使用MindSpore Lite进行模型训练相关的部分接口，具体请见下方表格
开发步骤
使用MindSpore Lite进行模型训练的开发流程如下图所示。
图 1 使用MindSpore Lite进行模型训练的开发流程
进入主要流程之前需要先引用相关的头文件，并编写函数生成随机的输入，具体如下：
#include <stdlib.h>\n#include <stdio.h>\n#include <string.h>\n#include \"mindspore/model.h\"\n\nint GenerateInputDataWithRandom(OH_AI_TensorHandleArray inputs) {\n  for (size_t i = 0; i < inputs.handle_num; ++i) {\n    float *input_data = (float *)OH_AI_TensorGetMutableData(inputs.handle_list[i]);\n    if (input_data == NULL) {\n      printf(\"OH_AI_TensorGetMutableData failed.\\n\");\n      return  OH_AI_STATUS_LITE_ERROR;\n    }\n    int64_t num = OH_AI_TensorGetElementNum(inputs.handle_list[i]);\n    const int divisor = 10;\n    for (size_t j = 0; j < num; j++) {\n      input_data[j] = (float)(rand() % divisor) / divisor;  // 0--0.9f\n    }\n  }\n  return OH_AI_STATUS_SUCCESS;\n}
然后进入主要的开发步骤，包括模型的准备、读取、编译、训练、模型导出和释放，具体开发过程及细节请见下文的开发步骤及示例。
模型准备。  准备的模型格式为.ms，本文以lenet_train.ms为例（此模型是提前准备的ms模型）。如果开发者需要使用自己准备的模型，可以按如下步骤操作： 首先基于MindSpore架构使用Python创建网络模型，并导出为.mindir文件，详细指南参考这里。然后将.mindir模型文件转换成.ms文件，转换操作步骤可以参考训练模型转换，.ms文件可以导入端侧设备并基于MindSpore端侧框架进行训练。 创建上下文，设置设备类型、训练配置等参数。 // Create and init context, add CPU device info\nOH_AI_ContextHandle context = OH_AI_ContextCreate();\nif (context == NULL) {\n    printf(\"OH_AI_ContextCreate failed.\\n\");\n    return OH_AI_STATUS_LITE_ERROR;\n}\n\nOH_AI_DeviceInfoHandle cpu_device_info = OH_AI_DeviceInfoCreate(OH_AI_DEVICETYPE_CPU);\nif (cpu_device_info == NULL) {\n    printf(\"OH_AI_DeviceInfoCreate failed.\\n\");\n    OH_AI_ContextDestroy(&context);\n    return OH_AI_STATUS_LITE_ERROR;\n}\nOH_AI_ContextAddDeviceInfo(context, cpu_device_info);\n\n// Create trainCfg\nOH_AI_TrainCfgHandle trainCfg = OH_AI_TrainCfgCreate();\nif (trainCfg == NULL) {\n    printf(\"OH_AI_TrainCfgCreate failed.\\n\");\n    OH_AI_ContextDestroy(&context);\n    return OH_AI_STATUS_LITE_ERROR;\n} 创建、加载与编译模型。  调用OH_AI_TrainModelBuildFromFile加载并编译模型。 // Create model\nOH_AI_ModelHandle model = OH_AI_ModelCreate();\nif (model == NULL) {\n    printf(\"OH_AI_ModelCreate failed.\\n\");\n    OH_AI_TrainCfgDestroy(&trainCfg);\n    OH_AI_ContextDestroy(&context);\n    return OH_AI_STATUS_LITE_ERROR;\n}\n\n// Build model\nint ret = OH_AI_TrainModelBuildFromFile(model, model_file, OH_AI_MODELTYPE_MINDIR, context, trainCfg);\nif (ret != OH_AI_STATUS_SUCCESS) {\n    printf(\"OH_AI_TrainModelBuildFromFile failed, ret: %d.\\n\", ret);\n    OH_AI_ModelDestroy(&model);\n    return ret;\n} 输入数据。  模型执行之前需要向输入的张量中填充数据。本例使用随机的数据对模型进行填充。 // Get Inputs\nOH_AI_TensorHandleArray inputs = OH_AI_ModelGetInputs(model);\nif (inputs.handle_list == NULL) {\n    printf(\"OH_AI_ModelGetInputs failed, ret: %d.\\n\", ret);\n    OH_AI_ModelDestroy(&model);\n    return ret;\n}\n\n// Generate random data as input data.\nret = GenerateInputDataWithRandom(inputs);\nif (ret != OH_AI_STATUS_SUCCESS) {\n    printf(\"GenerateInputDataWithRandom failed, ret: %d.\\n\", ret);\n    OH_AI_ModelDestroy(&model);\n    return ret;\n} 执行训练。  使用OH_AI_ModelSetTrainMode接口设置训练模式，使用OH_AI_RunStep接口进行模型训练。 // Set Traim Mode\nret = OH_AI_ModelSetTrainMode(model, true);\nif (ret != OH_AI_STATUS_SUCCESS) {\n    printf(\"OH_AI_ModelSetTrainMode failed, ret: %d.\\n\", ret);\n    OH_AI_ModelDestroy(&model);\n    return ret;\n}\n\n// Model Train Step\nret = OH_AI_RunStep(model, NULL, NULL);\nif (ret != OH_AI_STATUS_SUCCESS) {\n    printf(\"OH_AI_RunStep failed, ret: %d.\\n\", ret);\n    OH_AI_ModelDestroy(&model);\n    return ret;\n}\nprintf(\"Train Step Success.\\n\"); 导出训练后模型。  使用OH_AI_ExportModel接口导出训练后模型。 // Export Train Model\nret = OH_AI_ExportModel(model, OH_AI_MODELTYPE_MINDIR, export_train_model, OH_AI_NO_QUANT, false, NULL, 0);\nif (ret != OH_AI_STATUS_SUCCESS) {\n    printf(\"OH_AI_ExportModel train failed, ret: %d.\\n\", ret);\n    OH_AI_ModelDestroy(&model);\n    return ret;\n}\nprintf(\"Export Train Model Success.\\n\");\n\n// Export Inference Model\nret = OH_AI_ExportModel(model, OH_AI_MODELTYPE_MINDIR, export_infer_model, OH_AI_NO_QUANT, true, NULL, 0);\nif (ret != OH_AI_STATUS_SUCCESS) {\n    printf(\"OH_AI_ExportModel inference failed, ret: %d.\\n\", ret);\n    OH_AI_ModelDestroy(&model);\n    return ret;\n}\nprintf(\"Export Inference Model Success.\\n\"); 释放模型。  不再使用MindSpore Lite推理框架时，需要释放已经创建的模型。 // Delete model.\nOH_AI_ModelDestroy(&model); 
调测验证
编写CMakeLists.txt。 cmake_minimum_required(VERSION 3.14)\nproject(TrainDemo)\n\nadd_executable(train_demo main.c)\n\ntarget_link_libraries(\n        train_demo\n        mindspore_lite_ndk\n) 使用ohos-sdk交叉编译，需要对CMake设置native工具链路径，即：-DCMAKE_TOOLCHAIN_FILE=\"/xxx/native/build/cmake/ohos.toolchain.camke\"。 编译命令如下，其中OHOS_NDK需要设置为native工具链路径：   mkdir -p build\n\n  cd ./build || exit\n  OHOS_NDK=\"\"\n  cmake -G \"Unix Makefiles\" \\\n        -S ../ \\\n        -DCMAKE_TOOLCHAIN_FILE=\"$OHOS_NDK/build/cmake/ohos.toolchain.cmake\" \\\n        -DOHOS_ARCH=arm64-v8a \\\n        -DCMAKE_BUILD_TYPE=Release\n\n  make  运行编译的可执行程序。 使用hdc连接设备，并将train_demo和lenet_train.ms推送到设备中的相同目录。使用hdc shell进入设备，并进入train_demo所在的目录执行如下命令，即可得到结果。 ./train_demo ./lenet_train.ms export_train_model export_infer_model  得到如下输出： Train Step Success.\nExport Train Model Success.\nExport Inference Model Success.\nTensor name: Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense/BiasAdd-op121, tensor size is 80, elements num: 20.\noutput data is:\n0.000265 0.000231 0.000254 0.000269 0.000238 0.000228  在train_demo所在目录可以看到导出的两个模型文件：export_train_model.ms和export_infer_model.ms。 
完整示例
#include <stdlib.h>\n#include <stdio.h>\n#include <string.h>\n#include \"mindspore/model.h\"\n\nint GenerateInputDataWithRandom(OH_AI_TensorHandleArray inputs) {\n  for (size_t i = 0; i < inputs.handle_num; ++i) {\n    float *input_data = (float *)OH_AI_TensorGetMutableData(inputs.handle_list[i]);\n    if (input_data == NULL) {\n      printf(\"OH_AI_TensorGetMutableData failed.\\n\");\n      return  OH_AI_STATUS_LITE_ERROR;\n    }\n    int64_t num = OH_AI_TensorGetElementNum(inputs.handle_list[i]);\n    const int divisor = 10;\n    for (size_t j = 0; j < num; j++) {\n      input_data[j] = (float)(rand() % divisor) / divisor;  // 0--0.9f\n    }\n  }\n  return OH_AI_STATUS_SUCCESS;\n}\n\nint ModelPredict(char* model_file) {\n  // Create and init context, add CPU device info\n  OH_AI_ContextHandle context = OH_AI_ContextCreate();\n  if (context == NULL) {\n    printf(\"OH_AI_ContextCreate failed.\\n\");\n    return OH_AI_STATUS_LITE_ERROR;\n  }\n\n  OH_AI_DeviceInfoHandle cpu_device_info = OH_AI_DeviceInfoCreate(OH_AI_DEVICETYPE_CPU);\n  if (cpu_device_info == NULL) {\n    printf(\"OH_AI_DeviceInfoCreate failed.\\n\");\n    OH_AI_ContextDestroy(&context);\n    return OH_AI_STATUS_LITE_ERROR;\n  }\n  OH_AI_ContextAddDeviceInfo(context, cpu_device_info);\n\n  // Create model\n  OH_AI_ModelHandle model = OH_AI_ModelCreate();\n  if (model == NULL) {\n    printf(\"OH_AI_ModelCreate failed.\\n\");\n    OH_AI_ContextDestroy(&context);\n    return OH_AI_STATUS_LITE_ERROR;\n  }\n\n  // Build model\n  int ret = OH_AI_ModelBuildFromFile(model, model_file, OH_AI_MODELTYPE_MINDIR, context);\n  if (ret != OH_AI_STATUS_SUCCESS) {\n    printf(\"OH_AI_ModelBuildFromFile failed, ret: %d.\\n\", ret);\n    OH_AI_ModelDestroy(&model);\n    return ret;\n  }\n\n  // Get Inputs\n  OH_AI_TensorHandleArray inputs = OH_AI_ModelGetInputs(model);\n  if (inputs.handle_list == NULL) {\n    printf(\"OH_AI_ModelGetInputs failed, ret: %d.\\n\", ret);\n    OH_AI_ModelDestroy(&model);\n    return ret;\n  }\n\n  // Generate random data as input data.\n  ret = GenerateInputDataWithRandom(inputs);\n  if (ret != OH_AI_STATUS_SUCCESS) {\n    printf(\"GenerateInputDataWithRandom failed, ret: %d.\\n\", ret);\n    OH_AI_ModelDestroy(&model);\n    return ret;\n  }\n\n  // Model Predict\n  OH_AI_TensorHandleArray outputs;\n  ret = OH_AI_ModelPredict(model, inputs, &outputs, NULL, NULL);\n  if (ret != OH_AI_STATUS_SUCCESS) {\n    printf(\"MSModelPredict failed, ret: %d.\\n\", ret);\n    OH_AI_ModelDestroy(&model);\n    return ret;\n  }\n\n  // Print Output Tensor Data.\n  for (size_t i = 0; i < outputs.handle_num; ++i) {\n    OH_AI_TensorHandle tensor = outputs.handle_list[i];\n    int64_t element_num = OH_AI_TensorGetElementNum(tensor);\n    printf(\"Tensor name: %s, tensor size is %ld ,elements num: %ld.\\n\", OH_AI_TensorGetName(tensor),\n           OH_AI_TensorGetDataSize(tensor), element_num);\n    const float *data = (const float *)OH_AI_TensorGetData(tensor);\n    printf(\"output data is:\\n\");\n    const int max_print_num = 50;\n    for (int j = 0; j < element_num && j <= max_print_num; ++j) {\n      printf(\"%f \", data[j]);\n    }\n    printf(\"\\n\");\n  }\n\n  OH_AI_ModelDestroy(&model);\n  return OH_AI_STATUS_SUCCESS;\n}\n\nint TrainDemo(int argc, const char **argv) {\n  if (argc < 4) {\n    printf(\"Model file must be provided.\\n\");\n    printf(\"Export Train Model path must be provided.\\n\");\n    printf(\"Export Inference Model path must be provided.\\n\");\n    return OH_AI_STATUS_LITE_ERROR;\n  }\n  const char *model_file = argv[1];\n  const char *export_train_model = argv[2];\n  const char *export_infer_model = argv[3];\n\n  // Create and init context, add CPU device info\n  OH_AI_ContextHandle context = OH_AI_ContextCreate();\n  if (context == NULL) {\n    printf(\"OH_AI_ContextCreate failed.\\n\");\n    return OH_AI_STATUS_LITE_ERROR;\n  }\n\n  OH_AI_DeviceInfoHandle cpu_device_info = OH_AI_DeviceInfoCreate(OH_AI_DEVICETYPE_CPU);\n  if (cpu_device_info == NULL) {\n    printf(\"OH_AI_DeviceInfoCreate failed.\\n\");\n    OH_AI_ContextDestroy(&context);\n    return OH_AI_STATUS_LITE_ERROR;\n  }\n  OH_AI_ContextAddDeviceInfo(context, cpu_device_info);\n\n  // Create trainCfg\n  OH_AI_TrainCfgHandle trainCfg = OH_AI_TrainCfgCreate();\n  if (trainCfg == NULL) {\n    printf(\"OH_AI_TrainCfgCreate failed.\\n\");\n    OH_AI_ContextDestroy(&context);\n    return OH_AI_STATUS_LITE_ERROR;\n  }\n\n  // Create model\n  OH_AI_ModelHandle model = OH_AI_ModelCreate();\n  if (model == NULL) {\n    printf(\"OH_AI_ModelCreate failed.\\n\");\n    OH_AI_TrainCfgDestroy(&trainCfg);\n    OH_AI_ContextDestroy(&context);\n    return OH_AI_STATUS_LITE_ERROR;\n  }\n\n  // Build model\n  int ret = OH_AI_TrainModelBuildFromFile(model, model_file, OH_AI_MODELTYPE_MINDIR, context, trainCfg);\n  if (ret != OH_AI_STATUS_SUCCESS) {\n    printf(\"OH_AI_TrainModelBuildFromFile failed, ret: %d.\\n\", ret);\n    OH_AI_ModelDestroy(&model);\n    return ret;\n  }\n\n  // Get Inputs\n  OH_AI_TensorHandleArray inputs = OH_AI_ModelGetInputs(model);\n  if (inputs.handle_list == NULL) {\n    printf(\"OH_AI_ModelGetInputs failed, ret: %d.\\n\", ret);\n    OH_AI_ModelDestroy(&model);\n    return ret;\n  }\n\n  // Generate random data as input data.\n  ret = GenerateInputDataWithRandom(inputs);\n  if (ret != OH_AI_STATUS_SUCCESS) {\n    printf(\"GenerateInputDataWithRandom failed, ret: %d.\\n\", ret);\n    OH_AI_ModelDestroy(&model);\n    return ret;\n  }\n\n  // Set Traim Mode\n  ret = OH_AI_ModelSetTrainMode(model, true);\n  if (ret != OH_AI_STATUS_SUCCESS) {\n    printf(\"OH_AI_ModelSetTrainMode failed, ret: %d.\\n\", ret);\n    OH_AI_ModelDestroy(&model);\n    return ret;\n  }\n\n  // Model Train Step\n  ret = OH_AI_RunStep(model, NULL, NULL);\n  if (ret != OH_AI_STATUS_SUCCESS) {\n    printf(\"OH_AI_RunStep failed, ret: %d.\\n\", ret);\n    OH_AI_ModelDestroy(&model);\n    return ret;\n  }\n  printf(\"Train Step Success.\\n\");\n\n  // Export Train Model\n  ret = OH_AI_ExportModel(model, OH_AI_MODELTYPE_MINDIR, export_train_model, OH_AI_NO_QUANT, false, NULL, 0);\n  if (ret != OH_AI_STATUS_SUCCESS) {\n    printf(\"OH_AI_ExportModel train failed, ret: %d.\\n\", ret);\n    OH_AI_ModelDestroy(&model);\n    return ret;\n  }\n  printf(\"Export Train Model Success.\\n\");\n\n  // Export Inference Model\n  ret = OH_AI_ExportModel(model, OH_AI_MODELTYPE_MINDIR, export_infer_model, OH_AI_NO_QUANT, true, NULL, 0);\n  if (ret != OH_AI_STATUS_SUCCESS) {\n    printf(\"OH_AI_ExportModel inference failed, ret: %d.\\n\", ret);\n    OH_AI_ModelDestroy(&model);\n    return ret;\n  }\n  printf(\"Export Inference Model Success.\\n\");\n\n  // Delete model.\n  OH_AI_ModelDestroy(&model);\n\n  // Use The Exported Model to predict\n  ret = ModelPredict(strcat(export_infer_model, \".ms\"));\n  if (ret != OH_AI_STATUS_SUCCESS) {\n    printf(\"Exported Model to predict failed, ret: %d.\\n\", ret);\n    return ret;\n  }\n  return OH_AI_STATUS_SUCCESS;\n}\n\nint main(int argc, const char **argv) { return TrainDemo(argc, argv); }
